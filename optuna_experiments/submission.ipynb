{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./data\")\n",
    "output_path = Path(\"./submission\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)\n",
    "    return metrics.log_loss(y_true, y_pred, sample_weight=1 / nc[y_true], eps=1e-15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import compose, impute, pipeline, preprocessing\n",
    "\n",
    "\n",
    "def get_preprocess_pipeline(df, cont_cols, cat_cols, drop_cols):\n",
    "    \"\"\"\n",
    "    Returns a pipeline that performs the following transformations:\n",
    "    * Standard scaling\n",
    "    * Log transformation\n",
    "    * Reciprocal transformation\n",
    "    * Box-Cox transformation\n",
    "    * Yeo-Johnson transformation\n",
    "    * Categorical imputing\n",
    "    * Semi-constant feature binarization\n",
    "\n",
    "    Based on the EDA from https://www.kaggle.com/code/mateuszk013/icr-eda-balanced-learning-with-lgbm-xgb/notebook\n",
    "\n",
    "    :param df: The dataframe to be transformed.\n",
    "    :param cont_cols: The names of the continuous variables.\n",
    "    :param drop_cols: The names of the dependent variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify columns that doesn't follow a normal distribution\n",
    "    # find an appropriate transformation for them to follow a normal distribution\n",
    "    r2_scores = defaultdict(tuple)\n",
    "\n",
    "    for feature in cont_cols:\n",
    "        orig = df[feature].dropna()\n",
    "        _, (*_, R_orig) = stats.probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log) = stats.probplot(np.log(orig), rvalue=True)\n",
    "        _, (*_, R_sqrt) = stats.probplot(np.sqrt(orig), rvalue=True)\n",
    "        _, (*_, R_reci) = stats.probplot(np.reciprocal(orig), rvalue=True)\n",
    "        _, (*_, R_boxcox) = stats.probplot(stats.boxcox(orig)[0], rvalue=True)\n",
    "        _, (*_, R_yeojohn) = stats.probplot(stats.yeojohnson(orig)[0], rvalue=True)\n",
    "        r2_scores[feature] = (\n",
    "            R_orig * R_orig,\n",
    "            R_log * R_log,\n",
    "            R_sqrt * R_sqrt,\n",
    "            R_reci * R_reci,\n",
    "            R_boxcox * R_boxcox,\n",
    "            R_yeojohn * R_yeojohn,\n",
    "        )\n",
    "\n",
    "    r2_scores = pd.DataFrame(\n",
    "        r2_scores,\n",
    "        index=(\"Original\", \"Log\", \"Sqrt\", \"Reciprocal\", \"BoxCox\", \"YeoJohnson\"),\n",
    "    ).T\n",
    "\n",
    "    r2_scores[\"Winner\"] = r2_scores.idxmax(axis=1)\n",
    "\n",
    "    # Identify columns to be transformed\n",
    "    no_transform_cols = r2_scores.query(\"Winner == 'Original'\").index\n",
    "    log_transform_cols = r2_scores.query(\"Winner == 'Log'\").index\n",
    "    reciprocal_transform_cols = r2_scores.query(\"Winner == 'Reciprocal'\").index\n",
    "    boxcox_transform_cols = r2_scores.query(\"Winner == 'BoxCox'\").index\n",
    "    yeojohnson_transform_cols = r2_scores.query(\"Winner == 'YeoJohnson'\").index\n",
    "\n",
    "    # Identify columns that are constant or semi-constant\n",
    "    numeric_descr = df.drop(columns=drop_cols).describe().T\n",
    "    semi_constant_mask = np.isclose(numeric_descr[\"min\"], numeric_descr[\"50%\"])\n",
    "    semi_constant_descr = numeric_descr[semi_constant_mask]\n",
    "    semi_const_cols_thresholds = semi_constant_descr[\"50%\"].to_dict()\n",
    "\n",
    "    # List of columns to be transformed\n",
    "    semi_const_cols = semi_const_cols_thresholds.keys()\n",
    "    no_transform_cols = no_transform_cols.drop(semi_const_cols, errors=\"ignore\").to_list()\n",
    "    log_transform_cols = log_transform_cols.drop(semi_const_cols, errors=\"ignore\").to_list()\n",
    "    reciprocal_transform_cols = reciprocal_transform_cols.drop(semi_const_cols, errors=\"ignore\").to_list()\n",
    "    boxcox_transform_cols = boxcox_transform_cols.drop(semi_const_cols, errors=\"ignore\").to_list()\n",
    "    yeojohnson_transform_cols = yeojohnson_transform_cols.drop(semi_const_cols, errors=\"ignore\").to_list()\n",
    "\n",
    "    # Transformations\n",
    "    standard_scaling = (\n",
    "        preprocessing.StandardScaler(),\n",
    "        no_transform_cols,\n",
    "    )\n",
    "    log_transform = (\n",
    "        pipeline.make_pipeline(\n",
    "            preprocessing.FunctionTransformer(func=np.log, feature_names_out=\"one-to-one\"),\n",
    "            preprocessing.StandardScaler(),\n",
    "        ),\n",
    "        log_transform_cols,\n",
    "    )\n",
    "    reciprocal_transform = (\n",
    "        pipeline.make_pipeline(\n",
    "            preprocessing.FunctionTransformer(func=np.reciprocal, feature_names_out=\"one-to-one\"),\n",
    "            preprocessing.StandardScaler(),\n",
    "        ),\n",
    "        reciprocal_transform_cols,\n",
    "    )\n",
    "    boxcox_transform = (\n",
    "        preprocessing.PowerTransformer(method=\"box-cox\", standardize=True),\n",
    "        boxcox_transform_cols,\n",
    "    )\n",
    "    yeojohnson_transform = (\n",
    "        preprocessing.PowerTransformer(method=\"yeo-johnson\", standardize=True),\n",
    "        yeojohnson_transform_cols,\n",
    "    )\n",
    "\n",
    "    # Other transformations\n",
    "    categorical_imputing = (\n",
    "        pipeline.make_pipeline(\n",
    "            impute.SimpleImputer(strategy=\"most_frequent\"),\n",
    "            preprocessing.OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "        ),\n",
    "        cat_cols,  # type: ignore\n",
    "    )\n",
    "    semi_const_transforms = [\n",
    "        (\n",
    "            pipeline.make_pipeline(\n",
    "                impute.SimpleImputer(strategy=\"median\"),\n",
    "                preprocessing.Binarizer(threshold=thresh),\n",
    "            ),\n",
    "            [col],\n",
    "        )\n",
    "        for col, thresh in semi_const_cols_thresholds.items()\n",
    "    ]\n",
    "\n",
    "    return pipeline.make_pipeline(\n",
    "        compose.make_column_transformer(\n",
    "            standard_scaling,\n",
    "            log_transform,\n",
    "            reciprocal_transform,\n",
    "            boxcox_transform,\n",
    "            yeojohnson_transform,\n",
    "            categorical_imputing,\n",
    "            *semi_const_transforms,\n",
    "            remainder=\"drop\",\n",
    "            verbose_feature_names_out=False,\n",
    "        ),\n",
    "        impute.KNNImputer(n_neighbors=10, weights=\"distance\"),\n",
    "    ).set_output(transform=\"pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def resample(X, y):\n",
    "    sampler = SMOTE()\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370, 57), (247, 57))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fastai.tabular.core import cont_cat_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "train_df = pd.read_csv(path / \"train.csv\", index_col=\"Id\")\n",
    "\n",
    "drop_cols = [\"EJ\"]\n",
    "dep_vars = [\"Class\"]\n",
    "\n",
    "untrainable_cols = drop_cols + dep_vars\n",
    "\n",
    "# Drops the dep_vars before splitting categorical and continuous variables\n",
    "cont_names, cat_names = cont_cat_split(train_df, dep_var=untrainable_cols)\n",
    "\n",
    "train_df, test_df = model_selection.train_test_split(train_df, test_size=0.4, random_state=33)\n",
    "train_df.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = get_preprocess_pipeline(train_df, cont_names, cat_names, untrainable_cols)\n",
    "X_pre = preprocessor.fit_transform(train_df.drop(columns=untrainable_cols))\n",
    "train_df = pd.merge(X_pre, train_df[untrainable_cols], left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    colsample_bylevel=0.3,\n",
    "    colsample_bynode=0.7,\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0.6,\n",
    "    learning_rate=0.0344,\n",
    "    max_depth=3,\n",
    "    min_child_weight=0.5,\n",
    "    n_estimators=650,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    scale_pos_weight=5.5,\n",
    "    subsample=0.6,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=balanced_log_loss,\n",
    ")\n",
    "\n",
    "X = train_df.drop(columns=untrainable_cols, errors=\"ignore\")\n",
    "y = train_df[dep_vars]\n",
    "\n",
    "kfold = model_selection.RepeatedStratifiedKFold(n_splits=7, n_repeats=4)\n",
    "\n",
    "for idx in kfold.split(X, y):\n",
    "    train_idx, _ = idx\n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "\n",
    "    X_res, y_res = resample(X_train, y_train)\n",
    "\n",
    "    model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.0406\n",
      "Balanced log loss: 0.0558\n",
      "Accuracy: 0.9892\n",
      "Kappa: 0.9579\n",
      "F1: 0.9643\n"
     ]
    }
   ],
   "source": [
    "pred_probs = model.predict_proba(X)\n",
    "y_true = y.values.ravel()\n",
    "\n",
    "y_pred = pred_probs.argmax(axis=1)\n",
    "\n",
    "log_loss_val = metrics.log_loss(y_true, pred_probs)\n",
    "balanced_log_loss_val = balanced_log_loss(y_true, pred_probs)\n",
    "\n",
    "accuracy_val = metrics.accuracy_score(y_true, y_pred)\n",
    "kappa_val = metrics.cohen_kappa_score(y_true, y_pred)\n",
    "f1_val = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Log loss: {log_loss_val:.4f}\")\n",
    "print(f\"Balanced log loss: {balanced_log_loss_val:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "print(f\"Kappa: {kappa_val:.4f}\")\n",
    "print(f\"F1: {f1_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocessor.transform(test_df.drop(columns=untrainable_cols, errors=\"ignore\"))\n",
    "y_test = test_df[dep_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.1902\n",
      "Balanced log loss: 0.2166\n",
      "Accuracy: 0.9271\n",
      "Kappa: 0.7897\n",
      "F1: 0.8364\n"
     ]
    }
   ],
   "source": [
    "pred_probs = model.predict_proba(X_test)\n",
    "y_true = y_test.values.ravel()\n",
    "\n",
    "y_pred = pred_probs.argmax(axis=1)\n",
    "\n",
    "log_loss_val = metrics.log_loss(y_true, pred_probs)\n",
    "balanced_log_loss_val = balanced_log_loss(y_true, pred_probs)\n",
    "\n",
    "accuracy_val = metrics.accuracy_score(y_true, y_pred)\n",
    "kappa_val = metrics.cohen_kappa_score(y_true, y_pred)\n",
    "f1_val = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Log loss: {log_loss_val:.4f}\")\n",
    "print(f\"Balanced log loss: {balanced_log_loss_val:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "print(f\"Kappa: {kappa_val:.4f}\")\n",
    "print(f\"F1: {f1_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
